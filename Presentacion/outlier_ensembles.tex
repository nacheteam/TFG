\documentclass[10pt]{beamer}

\usepackage[utf8]{inputenc}
\usepackage[spanish, es-tabla]{babel}

\usetheme{metropolis}
\usepackage{appendixnumberbeamer}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}

\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{esvect}

\usepackage{multimedia}

\usepackage[spanish,onelanguage]{algorithm2e} %for psuedo code

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

\title{Detección de anomalías basada en técnicas de ensembles}
\author{Ignacio Aguilera Martos}
\date{\today}
\institute{Trabajo Fin de Grado \\ \href{https://github.com/nacheteam/Ensemble-Outlier-Analysis}{Código disponible en GitHub}}

\begin{document}

\maketitle

\begin{frame}[fragile]{Contenidos}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}

\section{Concepto de anomalía basado en distancias}

\begin{frame}[fragile]{Tukey's Fences}
\vspace{10px}
\pause
\metroset{block=fill}

Pensado para el caso uno-dimensional.

\pause

\begin{block}{Tukey's Fences}
	Valores fuera del rango $[Q_1 - k(Q_3 - Q_1), Q_3 + k(Q_3 - Q_1)]$ con $k=1.5$
\end{block}

\pause

La propuesta de $k=1.5$ es arbitraria.

\end{frame}

\begin{frame}[fragile]{Tukey's Fences}
\vspace{10px}
\metroset{block=fill}
\centering
\movie[height = 0.8\textheight, width=0.8\textwidth, poster, showcontrols]{}{Imagenes/outlier-1d.mp4}

\end{frame}

\begin{frame}[fragile]{Tukey's Fences}
\vspace{10px}
\metroset{block=fill}

\begin{figure}
	\centering
	\includegraphics[scale=0.6]{Imagenes/outlier-1d.png}
\end{figure}

\end{frame}

\begin{frame}[fragile]{Extensión al caso de mayor dimensionalidad}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{block}{Criterio}
	Aplicar el criterio de Tukey a cada una de las características.
	\pause
	\textbf{\underline{Trivial}}.
\end{block}

\pause

\begin{block}{Criterio de clusters}
	\begin{enumerate}
		\item Agrupamos los datos por clusters.
		\pause
		\item Encontramos el cluster más cercano para cada instancia.
		\pause
		\item Si la distancia del objeto al centroide del cluster es mayor que $1.5$ veces la mayor
		distancia intercluster entonces es una anomalía.
	\end{enumerate}
\end{block}

\end{frame}

\begin{frame}[fragile]{Ejemplo 1}
\vspace{10px}
\metroset{block=fill}
\centering
\movie[height = 0.8\textheight, width=0.8\textwidth, poster, showcontrols]{}{Imagenes/outlier-2d-case1.mp4}

\end{frame}

\begin{frame}[fragile]{Ejemplo 1}
\vspace{10px}
\metroset{block=fill}

\begin{figure}
	\centering
	\includegraphics[scale=0.6]{Imagenes/outlier-2d-case1.png}
\end{figure}

\end{frame}

\begin{frame}[fragile]{Ejemplo 2}
\vspace{10px}
\metroset{block=fill}
\centering
\movie[height = 0.8\textheight, width=0.8\textwidth, poster, showcontrols]{}{Imagenes/outlier-2d-case2.mp4}

\end{frame}

\begin{frame}[fragile]{Ejemplo 2}
\vspace{10px}
\metroset{block=fill}

\begin{figure}
\centering
\includegraphics[scale=0.6]{Imagenes/outlier-2d-case2.png}
\end{figure}

\end{frame}

\section{Aprendizaje Automático}

\begin{frame}[fragile]{Problema que abordamos}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{alertblock}{Problema de detección de anomalías}
	Es un problema de aprendizaje no supervisado pues no disponemos de las etiquetas.
\end{alertblock}

\pause

\begin{block}{Partes teóricas del problema}
	\begin{enumerate}
		\item Generador.
		\item Sistema.
		\item Máquina de aprendizaje.
	\end{enumerate}
\end{block}

\end{frame}

\begin{frame}[fragile]{Alta dimensionalidad}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{block}{Propiedades de conjuntos de alta dimensionalidad}
	\begin{enumerate}
		\item La densidad disminuye exponencialmente al aumentar la dimensionalidad.
		\pause
		\item Cuanto mayor es la dimensionalidad mayor debe ser el radio de una bola para englobar el mismo porcentaje de datos.
		\pause
		\item Casi todo punto está más cerca del borde del conjunto que de otro punto. 
		$$D(d,n) = (1-\frac{1}{2}^{\frac{1}{n}})^{\frac{1}{d}}$$
		\pause
		\item Casi todo punto es una anomalía sobre su propia proyección.
	\end{enumerate}
\end{block}

\pause

\begin{alertblock}{Maldición de la alta dimensionalidad}
	A mayor dimensionalidad mayor número de puntos necesitamos para obtener una aproximación con funciones de igual regularidad.
\end{alertblock}

\end{frame}

\begin{frame}[fragile]{Aproximación de funciones}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{alertblock}{Aproximación de funciones}
	Nuestro objetivo es aproximar la función de salida del sistema con los datos que tenemos.
\end{alertblock}

\pause

\begin{block}{Teoremas útiles}
	\begin{itemize}
		\item Teorema de Aproximación de Weierstrass
		\pause
		\item Serie de Fourier
		\pause
		\item Teorema de Kolmogorov-Arnold
	\end{itemize}
\end{block}

\end{frame}

\begin{frame}[fragile]{Teorema de Aproximación de Weierstrass}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{block}{Teorema de Aproximación de Weierstrass}
	Supongamos que tenemos una función $f:[a,b]\rightarrow \mathbb{R}$ continua. Entonces $\forall \epsilon > 0$ existe un polinomio $p$ tal que $\forall x \in [a,b]$ tenemos que $|f(x)-p(x)|<\epsilon$.
\end{block}

\pause

\begin{alertblock}{Aproximación por polinomios}
	El Teorema de Aproximación de Weierstrass nos da una forma de aproximar funciones por polinomios.
\end{alertblock}

\end{frame}

\begin{frame}[fragile]{Serie de Fourier}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{block}{Serie de Fourier}
	Si tenemos una función $f:\mathbb{R} \rightarrow \mathbb{R}$ integrable en el intervalo $[t_0 - \frac{T}{2}, t_0 + \frac{T}{2}]$ entonces se puede obtener el desarrollo de Fourier de $f$ en dicho intervalo. Si $f$ es periódica en toda la recta real la aproximación es válida en todos los valores en los que esté definida.
	
	$$f(t) \approx \frac{a_0}{2} + \sum_{n=1}^{\infty}\left[ a_n \cos (\frac{2n\pi}{T}t) + b_n \sin (\frac{2n\pi}{T}t) \right]$$
	
	$$a_0 = \frac{2}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}}f(t)dt, \ a_n = \frac{2}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} f(t) \cos (\frac{2n\pi}{T}t) dt,$$
	
	$$b_n = \frac{2}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}} f(t) \sin (\frac{2n\pi}{T}t) dt$$
\end{block}

\end{frame}

\begin{frame}[fragile]{Teorema de Kolmogorov-Arnold}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{block}{Teorema de Superposición Kolmogorov-Arnold}
	Sea $f$ una función continua de varias variables $f:X_1 \times ... \times X_n \rightarrow \mathbb{R}$, entonces existen funciones $\Phi_q : \mathbb{R}\rightarrow \mathbb{R}$ y $\phi_{q,p} : X_p \rightarrow [0,1]$ tales que $f$ se puede expresar como:
	
	$$f(x) = f(x_1, ..., x_n) = \sum_{q=0}^{2n}\Phi_q ( \sum_{p=1}^{n}\phi_{q,p}(x_p))$$
\end{block}

\pause

\begin{alertblock}{Dimensionalidad en datos y funciones}
	La maldición de la dimensionalidad es intrínseca a los datos. Este Teorema nos dice que la capacidad expresiva o complejidad de las funciones de una sola variable es la misma que las de varias variables.
\end{alertblock}

\end{frame}

\begin{frame}[fragile]{Equilibrio Sesgo-Varianza}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{block}{Notación}
	$$MSE = \frac{1}{n}\sum_{i=1}^{n}{y_i - g(X_i,\mathcal{D})}^2$$
	
	$$E[MSE] = \frac{1}{n}\sum_{i=1}^{n}E[{y_i - g(X_i,\mathcal{D})}^2] = \cdots$$
	
	$$ = \frac{1}{n}\sum_{i=1}^{n}\{ f(X_i) - E[g(X_i, \mathcal{D})] \}^2 + \frac{1}{n}\sum_{i=1}^{n}E[\{ E[g(X_i, \mathcal{D})] - g(X_i, \mathcal{D}) \}^2]$$
	
	$$ = sesgo^2 + varianza$$
\end{block}

\end{frame}

\begin{frame}[fragile]{Teoría del Aprendizaje}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{block}{Teorema clave de la Teoría del Aprendizaje}
	Para funciones de pérdida acotadas el principio inductivo de minimización del error empírico es consistente sí y sólo si el error empírico converge uniformemente al valor real del error en el siguiente sentido:
	
	$$\lim\limits_{n\rightarrow \infty} P[\sup_{\omega}|R(\omega) - R_{emp}(\omega)|>\epsilon] = 0 \ , \ \forall \epsilon >0$$
\end{block}

\end{frame}

\begin{frame}[fragile]{Cota ERM}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{block}{Dimensión VC}
	Decimos que un conjunto de funciones tiene dimensión VC $h$ si puede resolver de forma óptima todos los casos de tamaño $h$ pero existe al menos uno de tamaño $h+1$ que no puede resolver.
\end{block}

\pause

\begin{block}{Cota ERM}
	Con probabilidad $1-\eta$
	
	$$R(\omega) \leq R_{emp}( \omega) + \frac{\epsilon}{2} \biggl( 1+\sqrt{1+\frac{4\cdot R_{emp}(\omega)}{\epsilon}} \biggl)$$
	
	\pause
	
	$$\epsilon = a_1 \cdot \frac{h(\ln (\frac{a_2 n}{h})+1) - \ln (\frac{\eta}{4})}{n}$$
\end{block}

\end{frame}

\section{Probabilidad Multivariante}

\begin{frame}[fragile]{Contenidos de la sección}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{block}{Contenidos útiles}
	\begin{itemize}
		\item Vectores aleatorios.
		\pause
		\item Independencia.
		\pause
		\item Probabilidad condicionada (sucesos, variables y $\sigma$-álgebras) y sus propiedades.
		\pause
		\item Esperanza condicionada (sucesos, variables y $\sigma$-álgebras) y sus propiedades.
		\pause
		\item Desigualdades famosas.
	\end{itemize}
\end{block}

\end{frame}

\begin{frame}[fragile]{Desigualdad de Markov y Chebychev}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{block}{Desigualdad de Markov}
	Sea $X$ una variable aleatoria que toma valores no negativos. Entonces para cualquier constante $\alpha$ satisfaciendo $E[X]<\alpha$ se cumple que:
	
	$$P(X>\alpha)\leq \frac{E[X]}{\alpha}$$
\end{block}

\begin{block}{Desigualdad de Chebychev}
	Sea $X$ una variable aleatoria arbitraria. Entonces para cualquier constante $\alpha$ se tiene que:
	
	$$P(|X - E[X]|>\alpha)\leq \frac{Var[X]}{\alpha^2d}$$
\end{block}

\end{frame}

\begin{frame}[fragile]{Desigualdades famosas}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{block}{Desigualdades estudiadas}
	\begin{itemize}
		\item Desigualdad de Markov
		\item Desigualdad de Chebychev
		\item Cotas de Chernoff
		\item Desigualdad de Hoeffding
	\end{itemize}
\end{block}

\end{frame}

\section{Concepto probabilístico de anomalía}

\begin{frame}[fragile]{Notación}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{block}{Notación usada}
	$$X = \{ x_1 , ... , x_n \}, \ x_i = (x_{s_1} , ... , x_{s_d})$$
	
	\pause
	
	$$S = \{ s_i | s_i \in \{ s_1 , ... , s_d \} \ con \ i\in \Delta \}$$
	
	\pause
	
	$$X_S \ proyecci\acute{o}n \ de \ los \ datos \ en \ el \ subespacio \ S$$
	
	\pause
	
	$$p_{s_1 , ... , s_p}(x_{s_1} , ... , x_{s_p})$$
	
	\pause
	
	$$p_{s_i}(x_{s_i})$$
\end{block}

\end{frame}

\begin{frame}[fragile]{Definiciones}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{block}{Definición subespacio incorrelado}
	Decimos que un subespacio $S$ es un subespacio incorrelado si y sólo si:
	
	$$p_{s_1 , ... , s_p}(x_{s_1} , ... , {x_{s_p}}) = \prod_{i=1}^{p}p_{s_i}(x_{s_i})$$
\end{block}

\pause

\begin{block}{Definición anomalía no trivial}
	Decimos que un objeto $x_S$ es una anomalía no trivial respecto del subespacio $S$ si:
	
	$$p_{s_1 , ... , s_p}(x_{s_1} , ... , x_{s_p})\ll p_{esp}(x_{s_1} , ... , x_{s_p})$$
\end{block}

\pause

\begin{alertblock}{Relación entre conceptos de anomalía}
	Este concepto de anomalía es complementario.
\end{alertblock}

\end{frame}

\begin{frame}[fragile]{Ejemplo de anomalía}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{Imagenes/ejemplo_anomalia_probabilidad}
\end{figure}

\end{frame}

\section{Modelos implementados}

\begin{frame}[fragile]{Algoritmos de Ensamblaje}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{block}{Algoritmos de ensamblaje}
	\begin{itemize}
		\item Algoritmos secuenciales
		\pause
		\item Algoritmos independientes
	\end{itemize}
\end{block}

\pause

\begin{figure}
	\centering
	\includegraphics[scale=0.3]{Imagenes/ensamblaje-secuencial}
\end{figure}

\end{frame}

\begin{frame}[fragile]{Algoritmos de Ensamblaje}
\vspace{10px}
\metroset{block=fill}

\begin{block}{Algoritmos de ensamblaje}
	\begin{itemize}
		\item Algoritmos secuenciales

		\item Algoritmos independientes
	\end{itemize}
\end{block}

\begin{figure}
	\centering
	\includegraphics[scale=0.3]{Imagenes/ensamblaje-independiente}
\end{figure}

\end{frame}

\begin{frame}[fragile]{HICS}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{algorithm}[H]{\textbf{HICS}}
	
	\KwIn{D: dataset}
	
	$scores = [ \ ]$
	
	$sub = $ subespacios de alto contraste
	
	\ForEach{$S \in sub$}{
		
		Ajustamos un modelo con el algoritmo LOF con la proyección sobre $S$
		
		$scores = scores + puntaje \ LOF$
		
	}
	
	$scores = \frac{scores}{|sub|}$
	
	\KwOut{scores: puntajes}
	
\end{algorithm}

\end{frame}

\begin{frame}[fragile]{HICS}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{algorithm}[H]{\textbf{CalcularConstraste}}
	
	\KwIn{subespacio: subespacio, M: número de iteraciones del subsampling, $\alpha$: valor para obtener el tamaño de la muestra, $D$: conjunto de datos}
	
	$size = n \cdot \sqrt[|subespacio|]{\alpha}$
	
	$dev=0$
	
	\ForEach{$i\in [1,M]$}{
		
		$comp\_atr = aleatorio \ de \ subespacio$
		
		$sel\_obj = $ muestra aleatoria de $D$ de tamaño $size$
		
		$dev = dev + CalcularDev(comp\_atr, sel\_obj, subespacio, D)$
		
	}
	
	$dev = \frac{dev}{M}$
	
	\KwOut{$dev$: contraste}
	
\end{algorithm}

\end{frame}

\begin{frame}[fragile]{HICS}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{algorithm}[H]{\textbf{CalcularDev}}
	
	\KwIn{$comp\_atr$: atributo con el que comparar, $sel\_obj$: muestra seleccionada aleatoriamente, $subespacio$: subespacio sobre el que calcular la desviación, $D$: conjunto de datos}
	
	$max = 0$
	
	\ForEach{$d\in D$}{
		
		$cum_1 = \sum_{o\in D} o[comp\_atr]$ si $o[comp\_atr]<d[comp\_atr]$
		
		$cum_2 = \sum_{o\in sel\_obj} o[comp\_atr]$ si $o[comp\_atr]<d[comp\_atr]$
		
		$f_a = \frac{cum_1}{|D|}$
		
		$f_b = \frac{cum_2}{|D|}$
		
		$subs = |f_a - f_b|$
		
		\If{$subs>max$}{
			
			$max = subs$
			
		}
		
	}
	
	\KwOut{max: máxima desviación}
	
\end{algorithm}

\end{frame}

\begin{frame}[fragile]{OUTRES}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{algorithm}[H]
	
	\KwIn{o: instancia, S: subespacio}
	
	\ForEach{$i\in (D \setminus S)$}{
		
		$S' = S\cup \{ i \}$
		
		\If{$S'$ es relevante}{
			
			$den(o, S') = \frac{1}{n} \sum_{p\in AN(o,S')} K_e (\frac{dist_{S'}(o,p)}{\epsilon (|S'|)})$
			
			$dev(o,S') = \frac{\mu - den(o,S')}{2\sigma}$
			
			\If{$dev(o,S')\geq 1$}{
				
				$r(o) = r(o) \cdot \frac{den(o,S')}{dev(o,S')}$
				
			}
			
			$OUTRES(o,S')$
			
		}
		
		\Else{
			
			Para recursividad
			
		}
		
	}
	
	\KwOut{r: puntajes}
	
\end{algorithm}

\end{frame}

\begin{frame}[fragile]{OUTRES}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{block}{Subespacio relevante}
	Decimos que un subespacio es relevante si no está distribuido uniformemente.
\end{block}

\pause

\begin{alertblock}{Procedimiento}
	Si la proyección de los datos sobre un subespacio está distribuida según una uniforme entonces sus proyecciones en una dimensión también lo están.
\end{alertblock}

\end{frame}

\begin{frame}[fragile]{OUTRES}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{block}{Elementos involucrados}
	$$AN(o,S) = \{ p | dist_{S}(o,p)\leq \epsilon (|S|) \}$$
	
	\pause
	
	$$\epsilon (|S|) = 0.5 \cdot \frac{h_{optimal}(|S|)}{h_{optimal}(2)}$$
	
	\pause
	
	$$h_{optimal} (d) = (\frac{8\Gamma (\frac{d}{2} + 1)}{\pi^{\frac{d}{2}}}(d+4)(2\sqrt{\pi})^d) n^{\frac{-1}{d+4}}$$
	
	\pause
	
	$$K_{\epsilon}(x) = (1-x^2) \ \forall x<1$$
\end{block}

\end{frame}

\begin{frame}[fragile]{LODA}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{algorithm}[H]{\textbf{ProyeccionesAleatorias}}
	
	\KwIn{$d$: dimension, $D$: dataset, $k$: número de histogramas y proyecciones}
	
	$no\_neg = [\sqrt{d}]$
	
	$proyecciones = [ \ ]$
	
	\ForEach{$i\in [1,k]$}{
		
		$ind = $ $no\_neg$ indices aleatorios en $[0,d]$
		
		$proy = $ vector con ceros en todas las posiciones menos en ind, donde hay valores sacados de una normal $\mathcal{N}(0,1)$
		
		$proyecciones = [proecciones, proy]$
		
	}
	
	\KwOut{$proyecciones$: proyecciones}
	
\end{algorithm}

\end{frame}

\begin{frame}[fragile]{LODA}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{algorithm}[H]{\textbf{ObtenerHistogramas}}
	
	\KwIn{$D$: dataset, $\{w_i\}_{i=1}^{k}$: vectores de proyecciones, $k$: numero de histogramas y vectores de proyección}
	
	Inicializamos los histogramas $\{h_i\}_{i=1}^{k}$
	
	\For{$j=1 \rightarrow |D|$}{
		
		\For{$i=1\rightarrow k$}{
			
			$z_i = x_j^T w_i$	
			
			Actualiza el histograma $h_i$ con $z_i$
			
		}
		
	}
	
	\KwOut{$\{h_i\}_{i=1}^{k}$: histogramas}
	
\end{algorithm}

\end{frame}

\begin{frame}[fragile]{LODA}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{algorithm}[H]{\textbf{LODA}}
	
	\KwIn{$x$: instancia, $\{h_i\}_{i=1}^{k}$: histogramas, $\{w_i\}_{i=1}^{k}$: vectores de proyección}
	
	\For{$i=1\rightarrow k$}{
		
		$z_i = x^T w_i$
		
		Obtenemos $p_i = p_i (z_i)$ del histograma $h_i$
		
	}
	
	$f = \frac{-1}{k} \sum_{i=1}^{k}\log (p_i (z_i))$
	
	\KwOut{$f$: puntaje de anomalía de $x$}
	
\end{algorithm}

\end{frame}

\begin{frame}[fragile]{Mahalanobis Kernel}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{algorithm}[H]{\textbf{Mahalanobis Kernel}}
	\KwIn{$D$}
	
	$S = DD^T$.
	
	$S = Q\Delta^2 Q^T$.
	
	Almacenamos los vectores propios columna no negativos de $Q\Delta$ en una matriz $D'$
	
	Normalizamos $D'$ para que tenga media $0$ y varianza $1$.
	
	$vector\_media = media(D')$
	
	$puntuaciones = []$
	
	\ForEach{fila en D'}{
		$score = distancia(vector\_media , fila)$
		
		$puntuaciones = [puntuaciones, score]$
	}
	\KwOut{puntuaciones}
\end{algorithm}

\end{frame}

\begin{frame}[fragile]{Trinity}
\vspace{10px}
\pause
\metroset{block=fill}


\begin{block}{Componentes}
	\begin{enumerate}
		\item Componente basado en distancias: KNN con $k=5$
		\pause
		\item Componente basado en dependencia: Mahalanobis Kernel
		\pause
		\item Componente basado en densidad en subespacios: IForest
	\end{enumerate}
\end{block}

\begin{alertblock}
	Aplicamos en todos los componentes la técnica de subsampling.
\end{alertblock}

\end{frame}

\begin{frame}[fragile]{Trinity}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{algorithm}[H]{\textbf{Trinity}}
	\KwIn{$D$}
	
	Ejecutamos KNN con $k=5$ y guardamos el resultado en $E_1$
	
	Ejecutamos Mahalanobis Kernel y guardamos el resultado en $E_2$
	
	Ejecutamos IForest y guardamos el resultado en $E_3$
	
	Estandarizamos $E_1 , E_2 \ y \ E_3$ a media $0$ y varianza $1$
	
	Hacemos la media para obtener las puntuaciones
	
	\KwOut{puntuaciones}
\end{algorithm}

\end{frame}

\begin{frame}[fragile]{Implementación}
\vspace{10px}
\pause
\metroset{block=fill}

\begin{block}{Recursos usados}
	\begin{itemize}
		\item Implementación hecha en Python3
		\pause
		\item Disponible en GitHub
		\pause
		\item Documentada con Sphinx
	\end{itemize}
\end{block}

\end{frame}

\begin{frame}[standout]
	\LARGE{¿Preguntas?}
	\vspace{10px}
\end{frame}


\end{document}
