\chapter*{}
%\thispagestyle{empty}
%\cleardoublepage

%\thispagestyle{empty}

\input{portada/portada_2}



\cleardoublepage
\thispagestyle{empty}

\begin{center}
{\large\bfseries Detección de anomalías basada en técnicas de ensembles: Biblioteca de algoritmos}\\
\end{center}
\begin{center}
Ignacio Aguilera Martos\\
\end{center}

%\vspace{0.7cm}
\noindent{\textbf{Palabras clave}: anomalía, ensamblaje, python, hics, outres, loda, mahalanobis kernel, trinity, aprendizaje automático, estadística, probabilidad, multivariante}\\

\vspace{0.7cm}
\noindent{\textbf{Resumen}}\\

La detección de anomalías es un ámbito de estudio que está ganando relevancia por ser una parte interesante en el tratamiento de los datos. Actualmente hacemos un manejo y un uso de los datos cada vez más voraz y creciente, necesitando no sólo técnicas que nos permitan obtener información de ellos sino además preprocesamiento de los datos que haga que estas técnicas funcionen de forma más eficiente. 

Las anomalías no sólo son útiles en el preprocesamiento de los datos, también son interesantes en detección de eventos atípicos en los mismos. Por ejemplo, podemos aplicar esta detección en casos como detección de fraude en compras con tarjetas bancarias o por ejemplo en predicción de fallos en sistemas como los frenos de un coche o un camión.

Para ello en el trabajo haremos un breve repaso de cuáles son las herramientas teóricas que hacen que el trabajo de nuestros algoritmos y modelos sea consistente y funcione así como herramientas estadísticas y del ámbito de la probabilidad que explican el funcionamiento de alguno de los modelos. 

Finalmente el trabajo desembocará en la implementación de algunos de los modelos del estado del arte en el ámbito de la detección de anomalías y su comparativa con modelos considerados como clásicos o tradicionales. Es decir, pondremos en contraposición los modelos de ensamblaje con los tradicionales para estudiarlos comparativamente. Asimismo veremos algunas conclusiones sobre los modelos y una propuesta de trabajo futuro con la intención de mejorar la situación actual de los modelos de ensamblaje.

\cleardoublepage


\thispagestyle{empty}


\begin{center}
{\large\bfseries Outlier detection based in ensemble methods: Library implementation}\\
\end{center}
\begin{center}
Ignacio Aguilera Martos\\
\end{center}

%\vspace{0.7cm}
\noindent{\textbf{Keywords}: outlier, ensemble, python, hics, outres, loda, mahalanobis kernel, trinity, machine learning, statistics, probability, multivariate}\\

\vspace{0.7cm}
\noindent{\textbf{Abstract}}\\

Nowadays it's being more and more important the analysis of outliers in the area of data preprocessing. The way we are using the data is more and more greedy and we need better ways of mining the knowledge from the data. The use of outlier detection can lead to better performance of the Machine Learning models as we can now detect the outliers and eliminate them or treat them separately. Another approach is the detection of certain events due to the appearance of outliers. For example we can have a record of a certain bank and the credit card users. If someone gets his card stolen the way the thief buys and where the payments occur can lead to a detection of an anomalous way of using this card and therefore to inform the person that he could have his card stolen.

The anomaly detection could be useful as well in various field as medicine or event detection. We could think and measure the failure of the brake system of a truck or an earthquake as an anomaly in our datasets and we could detect them or even predict them. In medicine we could see the problem as finding what's wrong on a set of patients. We could think for example on a set of blood tests. One of them is very bad in terms of some of the values. If it is posed against healthy people the anomaly detection algorithm could give us not only the fact that the person is ill but also where the anomalous values are coming from to make it easier for the doctor to discover the problem.

The aim of this project is the discussion of ensemble models against the conventional ones in the discussion of the dilema of meta-algorithms versus simple algorithms. This study accomplishes the task of theoretical and practical discussion of the topic as well as further conclusions towards future work.

The way we introduced the outlier detection and study on this job has been preceded by a deep study on Machine Learning and the statistics it involves. This previous study is due to the importance of understanding why the algorithms and models implemented in this job are suitable for their use and therefore we can know as well their limits. This section has a description and introduction of the Machine Learning area and it evolves on the study of ERM and the in sample and out sample dilema in the field. A review on bias variance tradeoff is also given in this section. We need to consider that the problem we are handling is a non-supervised problem as in the general scenario we will not have the gound truth available, this is, the labels indicating which instance is an outlier and which isn't. That's why it is important to have a review and understand that variance and bias are put in a two hand balance, if we reduce variance we increase bias and viceversa.

After this study the first concept of outlier is given based on distances and quartiles. It is not only important the way we define the outliers but also why should we consider detecting them on our datasets before getting any further work done. Criterions and possibilities are described in this section as for example the Tukey's Fences criterion which is the most used one.

Probability, and even more multivariate probability, is very important in our study. Nearly all models implemented base their working principle on density or probability estimation. For this reason I've considered getting our hands firs on a multivariate introduction with useful concepts for explaining and understanding the model. This does not means that all concepts are going to be used in the study but, those included have been useful to me in the process of understanding how the theory of the models is built around. This section gets some introduction concepts of random vectors taking into account the definition and properties of random variables. It also touches the independence concept and some caracterizations involving random vectors. The most useful and interesting part of this section from the mathematical point of view is the conditional probability and conditional expectation. These concepts are relevant because they have not been introduced during the course of the degree and they are very important an related with Stochastic Processes. These concepts can apply to dynamically generated data as time series or stream flow data.

Due to this probability introduction a new outlier definition is available to us through these concepts. A probability-based definition of outlier is given and then used in serveral models as HICS or OUTRES. This probability definition involves probability density expectation, marginal distribution and joint distribution. This definition is not put against the distance based one, but to complement of fullfill the first definition with non trivial outliers. This concept is drawn from the OUTRES and HICS papers.

All this theory gives us way to approach the model implementation. The concept of ensemble appears here as the algorithms used in this study are no conventional ones. As the title says, the study goes around outlier ensembles. The concept is given here as the final goal is to make a reasonable comparison between the outlier detection algorithms, those called ensembles and what we could consider as traditional or conventional. We can consider a ensemble algorithm as an algorithm that makes use of simple algorithms combining them or algorithms that make a data transformation to study the dataset in a specific way. This includes not only doing projections to make for example histograms but also the study of the data in subspaces and applying different techniques or selection mechanisms depending on the subspace. The definition is made by giving the different types of ensembles based on Charu's Aggarwal book on outlier detection and outlier ensembles.

This portfolio contains the implementation and explanation of the five models chosen: HICS, OUTRES, Trinity, Mahalanobis Kernel and LODA. They are not chosen by coincidence but trying to cover most of the types of algorithms available in the state of art of outlier ensembles. HICS and OUTRES are from the subtype of ensembles called subspace miners of subspace based. The basic explanation of them is that they try to analyze the data in certain subspaces so they can measure the density in each one of them and compare it to the rest of the instances of the dataset. HICS mantaing the point of view of choosing the subspaces with no relation or conection with the instances of the dataset while OUTRES tries the other way around, this is choosing the subspace based on the instance we are considering. Mahalanobis Kernel is a PCA-based algorithm or at least PCA-influenced as it belongs to the same category. Trinity is a meta-model or meta-algorithm, this is a algorithm that combines simple models in order to obtain a more robust algorithm. Finally LODA uses the last technique I've chosen which is histogram-based. The short explanation of this type of model is the use of histograms to study the frequency of appearance of a certain value in the data.

So now we have all set up for our experiments. For the implementation Python has been chosen for several reasons. First of all Python is a versatile flexible language giving a lot of possibilities on the libraries you can use. Second we can make the development usefull for more people as Python is one of the most used languages on data science. For this reason I've decided to make myself a library with the algorithms implemented so they can be easily used by Python users and furthermore the development is completely open and available in GitHub so that anyone can see the code, understand it and fix bugs or extend the library content with new models if necessary.

All models are put against the same set of datasets given from the Stony Brooks University. This University mantains a set of datasets for outlier detection with the ground truth available so practitioners can work with it. These datasets allow us to treat the problem as a semi-supervised problem as we can obtain at the end a percentage of accuracy. The work first executes all models on these datasets to measure the performance of the ensemble models. To compare them with the classical ones we need the implementation of those classical. For this purpose the library PyOD is used. This library contains the implementation of a lot of models including some ensemble algorithms but not the ones I've chosen. With these implementations we can compare the accuracy and times of the models over the datasets. For a better understanding of the models they are analyzed individually if necessary.

So, I have ended up with a study of the state of art of ensemble algorithms putting them against the classical approach to see who outperforms whom. Future work is also discused at the end of this portfolio as the possibilities of improvement are still there due to the difficulty of the problem and the novelty of the ensemble thechniques.

\chapter*{Agradecimientos}
\thispagestyle{empty}

En primer lugar me gustaría agradecer a mi tutor, Francisco Herrera Triguero por su apoyo en el desarrollo de este trabajo. Ha sido un honor poder desarrollar este trabajo con su ayuda. Igualmente me gustaría agradecerle a Jacinto Carrasco Castillo su ayuda en el desarrollo del mismo y su dedicación conmigo.

Me gustaría agradecer el apoyo también de mis padres quienes me han acompañado durante estos 5 años y sin los cuales no podría haber completado este camino. Gracias por ser referentes para mí y no dejarme desistir en mis momentos más bajos.

A mis amigos y compañeros que han seguido el mismo camino que yo y que tanto apoyo me han dado para completar estas carreras. Luis, Pablo, Antonio, Diego, Darío, Iñaki y todos los que no puedo mencionar por no hacer estos agradecimientos infinitos. Gracias a todos por ser no sólo unos compañeros si no unos verdaderos amigos. También quiero agradecer a mis amigos fuera de la carrera. Alberto, Alex, Luismi, Pablo, Carmen, Elio y todos los que me dejo en el tintero, gracias por ayudarme y darme un empujón siempre que lo he necesitado.

A mi prima María y mi primo Fran quienes también me han servido de apoyo en esta carrera. A mis abuelos, Ramiro, Vicenta y María. No he podido tener unos mejores referentes que vosotros. Espero llegar a poder ser así algún día. Espero también haber cumplido las expectativas que pudierais tener sobre mí.

Y a tí, María. Gracias por hacer este camino lo más fácil posible, por tus ánimos, tu cariño y comprensión. Me has enseñado lo que es verdadero trabajo duro, perseverancia y mejorar como persona. No puedo más que agradecerte estos años juntos y espero me dejes seguir acompañándote y formando un equipo.

Gracias a todos, os quiero.
