\chapter*{}
%\thispagestyle{empty}
%\cleardoublepage

%\thispagestyle{empty}

\input{portada/portada_2}



\cleardoublepage
\thispagestyle{empty}

\begin{center}
{\large\bfseries Detección de anomalías basada en técnicas de ensembles: Biblioteca de algoritmos}\\
\end{center}
\begin{center}
Ignacio Aguilera Martos\\
\end{center}

%\vspace{0.7cm}
\noindent{\textbf{Palabras clave}: anomalía, ensamblaje, python, hics, outres, loda, mahalanobis kernel, trinity, aprendizaje automático, estadística}\\

\vspace{0.7cm}
\noindent{\textbf{Resumen}}\\

La detección de anomalías es un ámbito de estudio que está ganando relevancia por ser una parte interesante en el tratamiento de los datos. Actualmente hacemos un manejo y un uso de los datos cada vez más voraz y creciente, necesitando no sólo técnicas que nos permitan obtener información de ellos sino además preprocesamiento de los datos que haga que estas técnicas funcionen de forma más eficiente. 

Las anomalías no sólo son útiles en el preprocesamiento de los datos, también son interesantes en detección de eventos atípicos en los mismos. Por ejemplo, podemos aplicar esta detección en casos como detección de fraude en compras con tarjetas bancarias o por ejemplo en predicción de fallos en sistemas como los frenos de un coche o un camión.

Para ello en el trabajo haremos un breve repaso de cuáles son las herramientas teóricas que hacen que el trabajo de nuestros algoritmos y modelos sea consistente y funcione así como herramientas estadísticas y del ámbito de la probabilidad que explican el funcionamiento de alguno de los modelos. 

Finalmente el trabajo desembocará en la implementación de algunos de los modelos del estado del arte en el ámbito de la detección de anomalías y su comparativa con modelos considerados como clásicos o tradicionales. Es decir, pondremos en contraposición los modelos de ensamblaje con los tradicionales para estudiarlos comparativamente. Asimismo veremos algunas conclusiones sobre los modelos y una propuesta de trabajo futuro con la intención de mejorar la situación actual de los modelos de ensamblaje.

\cleardoublepage


\thispagestyle{empty}


\begin{center}
{\large\bfseries Outlier detection based in ensemble methods: Library implementation}\\
\end{center}
\begin{center}
Ignacio Aguilera Martos\\
\end{center}

%\vspace{0.7cm}
\noindent{\textbf{Keywords}: outlier, ensemble, python, hics, outres, loda, mahalanobis kernel, trinity, machine learning, statistics}\\

\vspace{0.7cm}
\noindent{\textbf{Abstract}}\\

Nowadays it's being more and more important the analysis of outliers in the area of data preprocessing. The way we are using the data is more and more greedy and we need better ways of mining the knowledge from the data. The use of outlier detection can lead to better performance of the Machine Learning models as we can now detect the outliers and eliminate them or treat them separately. Another approach is the detection of certain events due to the appearance of outliers. For example we can have a record of a certain bank and the credit card users. If someone gets his card stolen the way the thief buys and where the payments occur can lead to a detection of an anomalous way of using this card and therefore to inform the person that he could have his card stolen.

The anomaly detection could be useful as well in various field as medicine or event detection. We could think and measure the failure of the brake system of a truck or an earthquake as an anomaly in our datasets and we could detect them or event predict them.

The aim of this project is the discussion of ensemble models against the conventional ones in the discussion of the dilema of meta-algorithms versus simple algorithms. This study accomplishes the task of theoretical and practical discussion of the topic as well as further conclusions towards future work.

The way we introduced the outlier detection and study on this job has been preceded by a deep study on Machine Learning and the statistics it envolves. This previous study is due to the importance of understanding why the algorithms and models implemented in this job are suitable for their use and therefore we can know as well their limits. This section has a description and introduction of the Machine Learning area and it evolves on the study of ERM and the in sample and out sample dilema in the field.

After this study the first concept of outlier is given based on distances and quartiles. It is not only important the way we define the outliers but also why should we consider detecting them on our datasets before getting any further work done. Criterions and possibilities are described in this section. 

Probability, and even more multivariate probability, is very important in our study. Nearly all models implemented base their working principple on density or probability estimation. For this reason I've considered getting our hands firs on a multivariate introduction with useful concepts for explaining and understanding the model. This does not means that all concepts are going to be used in the study but, those included have been useful to me in the process of understanding how the theory of the models is built around. 

Due to this probability introduction a new outlier definition is available to us through these concepts. A probability-based definition of outlier is given and then used in serveral models as HICS or OUTRES. This probability definition involves probability density expectation, marginal distribution and joint distribution. This definition is not put against the distance based one, but to complement of fullfill the first definition with non trivial outliers.

All this theory gives us way to approach the model implementation. The concept of ensemble appears here as the algorithms used in this study are no conventional ones. As the title says, the study goes around outlier ensembles. The concept is given here as the final goal is to make a reasonable comparison between the outlier detection algorithms, those called ensembles and what we could consider as traditional or conventional. 

This portfolio contains the implementation and explanation of the five models chosen: HICS, OUTRES, Trinity, Mahalanobis Kernel and LODA. They are not chosen by coincidence but trying to cover most of the types of algorithms avaiable in the state of art of outlier ensembles. HICS and OUTRES are from the subtype of ensembles called subspace miners of subspace based. The basic explanation of them is that they try to analyze the data in certain subspaces so they can measure the density in each one of them and compare it to the rest of the instances of the dataset. HICS mantaing the point of view of choosing the subspaces with no relation or conection with the instances of the dataset while OUTRES tries the other way around, this is choosing the subspace based on the instance we are considering. Mahalanobis Kernel is a PCA-based algorithm or at leats PCA-influenced as it belongs to the same category. Trinity is a meta-model or meta-algorithm, this is a algorithm that combines simple models in order to obtain a more robust algorithm. Finally LODA uses the last technique I've chosen which is histogram-based. The short explanation of this type of model is the use of histograms to study the frequency of appearance of a certain data of value.

So now we have all set up for our experiments. For the implementation Python has been chosen for several reasons. First of all Python is a versatile flexible language giving a lot of possibilities on the libraries you can use. Second we can make the development usefull for more people as Python is one of the most used languages on data science. For this reason I've decided to make myself a library with the algorithms implemented so they can be easily used by Python users and furthermore the development is completely open and available in GitHub so that anyone can see the code, understand it and fix bugs or extend the library content with new models if necessary.

All models are put against the same set of datasets given from the Stony Brooks University. This University mantains a set of datasets for outlier detection with the ground truth available so practitioners can work with it. These datasets allow us to treat the problem as a semi-supervised problem as we can obtain at the end a percentage of accuracy. The work first executes all models on these datasets to measure the performance of the ensemble models. To compare them with the classical ones we need the implementation of those classical. For this purpose the library PyOD is used. This library contains the implementation of a lot of models including some ensemble algorithms but not the ones I've chosen. With these implementations we can compare the accuracy and times of the models over the datasets. For a better understanding of the models they are analyzed individually if necessary.

So, I have ended up with a study of the state of art of ensemble algorithms putting them against the classical approach to see who outperforms whom. Future work is also discused at the end of this portfolio as the possibilities of improvement are still there due to the difficulty of the problem.

\chapter*{Agradecimientos}
\thispagestyle{empty}

       \vspace{1cm}


Poner aquí agradecimientos...

