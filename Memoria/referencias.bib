
@book{yaser_learning_2012,
	title = {Learning from Data: a short course},
	url = {https://work.caltech.edu/telecourse.html},
	shorttitle = {Learning from Data},
	pagetotal = {215},
	author = {Yaser, Abu-Mostafa and Malik, Magdon-Ismail and Hsuan-Tien, Lin},
	date = {2012},
	langid = {english}
}

@book{cherkassky_learning_2007,
	title = {Learning from Data: Concepts, Theory, and Methods},
	isbn = {978-0-470-14051-2},
	shorttitle = {Learning from Data},
	abstract = {An interdisciplinary framework for learning methodologies—covering statistics, neural networks, and fuzzy logic, this book provides a unified treatment of the principles and methods for learning dependencies from data. It establishes a general conceptual framework in which various learning methods from statistics, neural networks, and fuzzy logic can be applied—showing that a few fundamental principles underlie most new methods being proposed today in statistics, engineering, and computer science. Complete with over one hundred illustrations, case studies, and examples making this an invaluable text.},
	pagetotal = {558},
	publisher = {John Wiley \& Sons},
	author = {Cherkassky, Vladimir and Mulier, Filip M.},
	date = {2007-09-10},
	langid = {english},
	note = {02476},
	keywords = {Computers / Data Modeling \& Design, Technology \& Engineering / Electrical}
}

@article{micenkova_learning_2015,
	title = {Learning Representations for Outlier Detection on a Budget},
	url = {http://arxiv.org/abs/1507.08104},
	abstract = {The problem of detecting a small number of outliers in a large dataset is an important task in many fields from fraud detection to high-energy physics. Two approaches have emerged to tackle this problem: unsupervised and supervised. Supervised approaches require a sufficient amount of labeled data and are challenged by novel types of outliers and inherent class imbalance, whereas unsupervised methods do not take advantage of available labeled training examples and often exhibit poorer predictive performance. We propose {BORE} (a Bagged Outlier Representation Ensemble) which uses unsupervised outlier scoring functions ({OSFs}) as features in a supervised learning framework. {BORE} is able to adapt to arbitrary {OSF} feature representations, to the imbalance in labeled data as well as to prediction-time constraints on computational cost. We demonstrate the good performance of {BORE} compared to a variety of competing methods in the non-budgeted and the budgeted outlier detection problem on 12 real-world datasets.},
	journaltitle = {{arXiv}:1507.08104 [cs]},
	author = {Micenková, Barbora and {McWilliams}, Brian and Assent, Ira},
	urldate = {2019-05-02},
	date = {2015-07-29},
	eprinttype = {arxiv},
	eprint = {1507.08104},
	note = {00000 },
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\nache\\Zotero\\storage\\929XJIER\\1507.html:text/html}
}

@article{kuncheva_theoretical_2002,
	title = {A theoretical study on six classifier fusion strategies},
	volume = {24},
	issn = {0162-8828},
	doi = {10.1109/34.982906},
	abstract = {We look at a single point in feature space, two classes, and L classifiers estimating the posterior probability for class /spl omega//sub 1/. Assuming that the estimates are independent and identically distributed (normal or uniform), we give formulas for the classification error for the following fusion methods: average, minimum, maximum, median, majority vote, and oracle.},
	pages = {281--286},
	number = {2},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kuncheva, L. I.},
	date = {2002-02},
	note = {00000},
	keywords = {classification error, classifier combination, classifier fusion strategies, Diversity reception, Error analysis, estimation theory, feature space, fusion methods, Gaussian distribution, identically distributed estimates, independent classifiers, majority vote, optimisation, order statistics, pattern classification, Pattern recognition, posterior probability, probability, Probability, Statistical distributions, theoretical error, Voting},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\nache\\Zotero\\storage\\SXL69LX4\\982906.html:text/html}
}

@article{galar_review_2012,
	title = {A Review on Ensembles for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches},
	volume = {42},
	issn = {1094-6977},
	doi = {10.1109/TSMCC.2011.2161285},
	shorttitle = {A Review on Ensembles for the Class Imbalance Problem},
	abstract = {Classifier learning with data-sets that suffer from imbalanced class distributions is a challenging problem in data mining community. This issue occurs when the number of examples that represent one class is much lower than the ones of the other classes. Its presence in many real-world applications has brought along a growth of attention from researchers. In machine learning, the ensemble of classifiers are known to increase the accuracy of single classifiers by combining several of them, but neither of these learning techniques alone solve the class imbalance problem, to deal with this issue the ensemble learning algorithms have to be designed specifically. In this paper, our aim is to review the state of the art on ensemble techniques in the framework of imbalanced data-sets, with focus on two-class problems. We propose a taxonomy for ensemble-based methods to address the class imbalance where each proposal can be categorized depending on the inner ensemble methodology in which it is based. In addition, we develop a thorough empirical comparison by the consideration of the most significant published approaches, within the families of the taxonomy proposed, to show whether any of them makes a difference. This comparison has shown the good behavior of the simplest approaches which combine random undersampling techniques with bagging or boosting ensembles. In addition, the positive synergy between sampling techniques and bagging has stood out. Furthermore, our results show empirically that ensemble-based algorithms are worthwhile since they outperform the mere use of preprocessing techniques before learning the classifier, therefore justifying the increase of complexity by means of a significant enhancement of the results.},
	pages = {463--484},
	number = {4},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	author = {Galar, M. and Fernandez, A. and Barrenechea, E. and Bustince, H. and Herrera, F.},
	date = {2012-07},
	note = {00000},
	keywords = {pattern classification, Accuracy, Algorithm design and analysis, Bagging, bagging-based approach, boosting, boosting-based approach, class distribution, class imbalance problem, classification, classifier ensemble, classifier learning, data mining, data mining community, ensemble learning algorithms, ensemble-based algorithms, ensemble-based method taxonomy, ensembles, hybrid-based approach, imbalanced data-sets, inner ensemble methodology, learning (artificial intelligence), Learning systems, machine learning, multiple classifier systems, Noise, preprocessing techniques, Proposals, random undersampling techniques, Training, two-class problems},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\nache\\Zotero\\storage\\Z2WT3M36\\5978225.html:text/html}
}

@inproceedings{sukhanov_dynamic_2019,
	title = {Dynamic Selection of Classifiers for Fusing Imbalanced Heterogeneous Data},
	doi = {10.1109/ICASSP.2019.8683482},
	abstract = {Data fusion ({DF}) from multiple heterogeneous sources is a typical task for many multisensor applications including remote sensing classification problems. Multiple classifier systems ({MCS}) provide a natural way to solve {DF} on the decision level by training individual classifiers separately on its own data source and then combine their outputs. In this paper, we consider a dynamic selection ({DS}) framework to select and fuse competent classifiers of {MCS}. For this, we propose a competence estimation and selection method to improve the performance of the {DF} system especially under class imbalance. We evaluate the method with synthetic and real datasets, demonstrating the applicability of the proposed framework.},
	eventtitle = {{ICASSP} 2019 - 2019 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {5361--5365},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Sukhanov, S. and Debes, C. and Zoubir, A. M.},
	date = {2019-05},
	note = {00000},
	keywords = {multiple classifier systems, data fusion, dynamic classifier selection, ensemble learning},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\nache\\Zotero\\storage\\7C7HWKQK\\8683482.html:text/html}
}

@article{carrega_simple_2019,
	title = {Simple continuous optimal regions of the space of data},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231219305314},
	doi = {10.1016/j.neucom.2019.03.081},
	abstract = {The definition of efficient programs for both maintenance and optimization is a struggling task in many industrial sectors. In this context, data analysis can significantly improve the state-of-the-art techniques, employed, for instance, to determine if a particular component or product is showing an anomalous behavior with respect to a defined nominal state. In fact, through the analysis of data collected on field, it is possible to detect optimal operating regions and to detect anomalies in advance. In this context, we propose a multi-purpose algorithm for unsupervised or semi-supervised learning in order to determine a simple continuous region of points. This region can be adopted in order to describe a component or a product nominal behavior and can be used in order to detect anomalies which are outside it. Such a region can be defined adopting a finite ensemble of thresholds, whose value can be physically interpreted. In order to show the effectiveness of our approach, the proposed method has been tested in an Anomaly Detection problem concerning Predictive Maintenance, exploiting data coming from a naval vessel, characterized by a combined diesel-electric and gas propulsion plant.},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Carrega, Alessio and Cipollini, Francesca and Oneto, Luca},
	urldate = {2019-04-23},
	date = {2019-04-11},
	note = {00000},
	keywords = {Anomaly detection, Machine learning, Optimal regions, Semi-supervised learning, Unsupervised learning},
	file = {ScienceDirect Snapshot:C\:\\Users\\nache\\Zotero\\storage\\NVLF8W8S\\S0925231219305314.html:text/html}
}

@article{cipollini_condition-based_2018,
	title = {Condition-based maintenance of naval propulsion systems: Data analysis with minimal feedback},
	volume = {177},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832017309973},
	doi = {10.1016/j.ress.2018.04.015},
	shorttitle = {Condition-based maintenance of naval propulsion systems},
	abstract = {The maintenance of the several components of a Ship Propulsion Systems is an onerous activity, which need to be efficiently programmed by a shipbuilding company in order to save time and money. The replacement policies of these components can be planned in a Condition-Based fashion, by predicting their decay state and thus proceed to substitution only when really needed. In this paper, authors propose several Data Analysis supervised and unsupervised techniques for the Condition-Based Maintenance of a vessel, characterised by a combined diesel-electric and gas propulsion plant. In particular, this analysis considers a scenario where the collection of vast amounts of labelled data containing the decay state of the components is unfeasible. In fact, the collection of labelled data requires a drydocking of the ship and the intervention of expert operators, which is usually an infrequent event. As a result, authors focus on methods which could allow only a minimal feedback from naval specialists, thus simplifying the dataset collection phase. Confidentiality constraints with the Navy require authors to use a real-data validated simulator and the dataset has been published for free use through the {OpenML} repository.},
	pages = {12--23},
	journaltitle = {Reliability Engineering \& System Safety},
	shortjournal = {Reliability Engineering \& System Safety},
	author = {Cipollini, Francesca and Oneto, Luca and Coraddu, Andrea and Murphy, Alan John and Anguita, Davide},
	urldate = {2019-04-23},
	date = {2018-09-01},
	note = {00005},
	keywords = {Unsupervised learning, Condition-based maintenance, Data analysis, Minimal feedback., Naval propulsion systems, Novelty detection, Supervised learning},
	file = {2018-Cipollini-Condition-based_maintenance_of_naval_propulsion_systems.pdf:C\:\\Users\\nache\\Zotero\\storage\\CE8GB4T2\\2018-Cipollini-Condition-based_maintenance_of_naval_propulsion_systems.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\nache\\Zotero\\storage\\JJKT4UXD\\S0951832017309973.html:text/html}
}

@inproceedings{zimek_subsampling_2013,
	title = {Subsampling for efficient and effective unsupervised outlier detection ensembles},
	pages = {428--436},
	booktitle = {Proceedings of the 19th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	publisher = {{ACM}},
	author = {Zimek, Arthur and Gaudet, Matthew and Campello, Ricardo {JGB} and Sander, Jörg},
	date = {2013},
	file = {2013-Zimek-Subsampling_for_efficient_and_effective_unsupervised_outlier_detection_ensembles.pdf:C\:\\Users\\nache\\Zotero\\storage\\QTNEUGUE\\2013-Zimek-Subsampling_for_efficient_and_effective_unsupervised_outlier_detection_ensembles.pdf:application/pdf;Snapshot:C\:\\Users\\nache\\Zotero\\storage\\82KVZY8G\\citation.html:text/html}
}

@article{aggarwal_theoretical_2015,
	title = {Theoretical foundations and algorithms for outlier ensembles},
	volume = {17},
	pages = {24--47},
	number = {1},
	journaltitle = {{ACM} {SIGKDD} Explorations Newsletter},
	author = {Aggarwal, Charu C. and Sathe, Saket},
	date = {2015},
	file = {2015-Aggarwal-Theoretical_Foundations_and_Algorithms_for_Outlier_Ensembles.pdf:C\:\\Users\\nache\\Zotero\\storage\\D9ZU5SK5\\2015-Aggarwal-Theoretical_Foundations_and_Algorithms_for_Outlier_Ensembles.pdf:application/pdf;Snapshot:C\:\\Users\\nache\\Zotero\\storage\\U9HW6JAP\\citation.html:text/html}
}

@article{zimek_survey_2012,
	title = {A survey on unsupervised outlier detection in high-dimensional numerical data},
	volume = {5},
	pages = {363--387},
	number = {5},
	journaltitle = {Statistical Analysis and Data Mining},
	author = {Zimek, Arthur and Schubert, Erich and Kriegel, Hans-Peter},
	date = {2012},
	file = {2012-Zimek-A_survey_on_unsupervised_outlier_detection_in_high-dimensional_numerical_data.pdf:C\:\\Users\\nache\\Zotero\\storage\\CKQJV7YR\\2012-Zimek-A_survey_on_unsupervised_outlier_detection_in_high-dimensional_numerical_data.pdf:application/pdf;Snapshot:C\:\\Users\\nache\\Zotero\\storage\\XU6RYCAH\\sam.html:text/html}
}

@inproceedings{lazarevic_feature_2005,
	title = {Feature bagging for outlier detection},
	pages = {157--166},
	booktitle = {Proceedings of the eleventh {ACM} {SIGKDD} international conference on Knowledge discovery in data mining},
	publisher = {{ACM}},
	author = {Lazarevic, Aleksandar and Kumar, Vipin},
	date = {2005},
	keywords = {bagging, detection rate, false alarm, feature subsets, integration, outlier detection},
	file = {2005-Lazarevic-Feature_bagging_for_outlier_detection.pdf:C\:\\Users\\nache\\Zotero\\storage\\G5HQZDJ4\\2005-Lazarevic-Feature_bagging_for_outlier_detection.pdf:application/pdf}
}

@article{zhao_lscp:_2018,
	title = {{LSCP}: Locally Selective Combination in Parallel Outlier Ensembles},
	shorttitle = {{LSCP}},
	journaltitle = {{arXiv} preprint {arXiv}:1812.01528},
	author = {Zhao, Yue and Hryniewicki, Maciej K. and Nasrullah, Zain and Li, Zheng},
	date = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval, Statistics - Machine Learning},
	file = {2018-Zhao-LSCP.pdf:C\:\\Users\\nache\\Zotero\\storage\\XXW7MR3Q\\2018-Zhao-LSCP.pdf:application/pdf}
}

@inproceedings{zhao_xgbod:_2018,
	title = {{XGBOD}: improving supervised outlier detection with unsupervised representation learning},
	shorttitle = {{XGBOD}},
	pages = {1--8},
	booktitle = {2018 International Joint Conference on Neural Networks ({IJCNN})},
	publisher = {{IEEE}},
	author = {Zhao, Yue and Hryniewicki, Maciej K.},
	date = {2018},
	keywords = {feature space, pattern classification, data mining, outlier detection, anomaly detection, embedded supervised classifier, ensemble methods, extreme gradient boosting outlier detection, hybrid approach, outlier ensemble, representation learning, semi-supervised machine learning, semisupervised ensemble algorithm, stacking, supervised machine learning methods, supervised outlier detection, unsupervised learning, unsupervised machine learning methods, unsupervised outlier mining algorithms, unsupervised representation learning, {XGBOD}},
	file = {2018-Zhao-XGBOD.pdf:C\:\\Users\\nache\\Zotero\\storage\\3N2ULHWL\\2018-Zhao-XGBOD.pdf:application/pdf}
}

@inproceedings{zhao_dcso:_2018,
	title = {{DCSO}: dynamic combination of detector scores for outlier ensembles},
	shorttitle = {{DCSO}},
	booktitle = {{ACM} {SIGKDD} Conference on Knowledge Discovery and Data Mining ({KDD}), Outlier Detection De-constructed Workshop, London, {UK}},
	author = {Zhao, Yue and Hryniewicki, Maciej K.},
	date = {2018},
	file = {2018-Zhao-DCSO.pdf:C\:\\Users\\nache\\Zotero\\storage\\X7F2B5MF\\2018-Zhao-DCSO.pdf:application/pdf}
}

@inproceedings{he_unified_2005,
	title = {A unified subspace outlier ensemble framework for outlier detection},
	pages = {632--637},
	booktitle = {International Conference on Web-Age Information Management},
	publisher = {Springer},
	author = {He, Zengyou and Deng, Shengchun and Xu, Xiaofei},
	date = {2005},
	file = {2005-He-A_Unified_Subspace_Outlier_Ensemble_Framework_for_Outlier_Detection.pdf:C\:\\Users\\nache\\Zotero\\storage\\E699J3CC\\2005-He-A_Unified_Subspace_Outlier_Ensemble_Framework_for_Outlier_Detection.pdf:application/pdf}
}

@article{zhao_pyod:_2019,
	title = {{PyOD}: A Python Toolbox for Scalable Outlier Detection},
	url = {http://arxiv.org/abs/1901.01588},
	shorttitle = {{PyOD}},
	abstract = {{PyOD} is an open-source Python toolbox for performing scalable outlier detection on multivariate data. Uniquely, it provides access to a wide range of outlier detection algorithms, including established outlier ensembles and more recent neural network-based approaches, under a single, well-documented {API} designed for use by both practitioners and researchers. With robustness and scalability in mind, best practices such as unit testing, continuous integration, code coverage, maintainability checks, interactive examples and parallelization are emphasized as core components in the toolbox's development. {PyOD} is compatible with both Python 2 and 3 and can be installed through Python Package Index ({PyPI}) or https://github.com/yzhao062/pyod.},
	journaltitle = {{arXiv}:1901.01588 [cs, stat]},
	author = {Zhao, Yue and Nasrullah, Zain and Li, Zheng},
	urldate = {2019-01-30},
	date = {2019-01-06},
	eprinttype = {arxiv},
	eprint = {1901.01588},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval, Statistics - Machine Learning},
	file = {2019-Zhao-PyOD.pdf:C\:\\Users\\nache\\Zotero\\storage\\TJSLAAZ7\\2019-Zhao-PyOD.pdf:application/pdf}
}

@article{strehl_cluster_2002,
	title = {Cluster Ensembles --- A Knowledge Reuse Framework for Combining Multiple Partitions},
	volume = {3},
	issn = {{ISSN} 1533-7928},
	url = {http://www.jmlr.org/papers/v3/strehl02a.html},
	pages = {583--617},
	issue = {Dec},
	journaltitle = {Journal of Machine Learning Research},
	author = {Strehl, Alexander and Ghosh, Joydeep},
	urldate = {2019-03-13},
	date = {2002},
	file = {2002-Strehl-Cluster_Ensembles_---_A_Knowledge_Reuse_Framework_for_Combining_Multiple.pdf:C\:\\Users\\nache\\Zotero\\storage\\FRJEX8MY\\2002-Strehl-Cluster_Ensembles_---_A_Knowledge_Reuse_Framework_for_Combining_Multiple.pdf:application/pdf}
}

@inproceedings{aggarwal_outlier_2001,
	location = {New York, {NY}, {USA}},
	title = {Outlier Detection for High Dimensional Data},
	isbn = {978-1-58113-332-5},
	url = {http://doi.acm.org/10.1145/375663.375668},
	doi = {10.1145/375663.375668},
	series = {{SIGMOD} '01},
	abstract = {The outlier detection problem has important applications in the field of fraud detection, network robustness analysis, and intrusion detection. Most such applications are high dimensional domains in which the data can contain hundreds of dimensions. Many recent algorithms use concepts of proximity in order to find outliers based on their relationship to the rest of the data. However, in high dimensional space, the data is sparse and the notion of proximity fails to retain its meaningfulness. In fact, the sparsity of high dimensional data implies that every point is an almost equally good outlier from the perspective of proximity-based definitions. Consequently, for high dimensional data, the notion of finding meaningful outliers becomes substantially more complex and non-obvious. In this paper, we discuss new techniques for outlier detection which find the outliers by studying the behavior of projections from the data set.},
	pages = {37--46},
	booktitle = {Proceedings of the 2001 {ACM} {SIGMOD} International Conference on Management of Data},
	publisher = {{ACM}},
	author = {Aggarwal, Charu C. and Yu, Philip S.},
	urldate = {2019-03-13},
	date = {2001},
	note = {event-place: Santa Barbara, California, {USA}},
	file = {2001-Aggarwal-Outlier_Detection_for_High_Dimensional_Data.pdf:C\:\\Users\\nache\\Zotero\\storage\\WLAW2NXJ\\2001-Aggarwal-Outlier_Detection_for_High_Dimensional_Data.pdf:application/pdf}
}

@inproceedings{muller_statistical_2011,
	title = {Statistical selection of relevant subspace projections for outlier ranking},
	pages = {434--445},
	booktitle = {Data Engineering ({ICDE}), 2011 {IEEE} 27th International Conference on},
	publisher = {{IEEE}},
	author = {Müller, Emmanuel and Schiffer, Matthias and Seidl, Thomas},
	date = {2011},
	keywords = {data analysis, outlier ranking, statistical selection, subspace projection, synthetic data, Variable speed drives},
	file = {2011-Müller-Statistical_selection_of_relevant_subspace_projections_for_outlier_ranking.pdf:C\:\\Users\\nache\\Zotero\\storage\\XB32HA4L\\2011-Müller-Statistical_selection_of_relevant_subspace_projections_for_outlier_ranking.pdf:application/pdf}
}

@inproceedings{liu_detecting_2010,
	title = {On detecting clustered anomalies using {SCiForest}},
	pages = {274--290},
	booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
	publisher = {Springer},
	author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
	date = {2010},
	keywords = {Anomaly Detector, Area Under Receiver Operate Characteristic Curve, Distance Factor, Normal Cluster, Outlier Detection},
	file = {2010-Liu-On_Detecting_Clustered_Anomalies_Using_SCiForest.pdf:C\:\\Users\\nache\\Zotero\\storage\\IU7C6LBA\\2010-Liu-On_Detecting_Clustered_Anomalies_Using_SCiForest.pdf:application/pdf}
}

@article{zhu_commentary:_2016,
	title = {Commentary: a decomposition of the outlier detection problem into a set of supervised learning problems},
	volume = {105},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-016-5566-8},
	doi = {10.1007/s10994-016-5566-8},
	shorttitle = {Commentary},
	abstract = {This article discusses the material in relation to {iForest} (Liu et al. in {ACM} Trans Knowl Discov Data 6(1):3, 2012) reported in a recent Machine Learning Journal paper by Paulheim and Meusel (Mach Learn 100(2–3):509–531, 2015). It presents an empirical comparison result of {iForest} using the default parameter settings suggested by its creator (Liu et al. 2012) and {iForest} using the settings employed by Paulheim and Meusel (2015). This comparison has an impact on the conclusion made by Paulheim and Meusel (2015).},
	pages = {301--304},
	number = {2},
	journaltitle = {Machine Learning},
	author = {Zhu, Ye and Ting, Kai Ming},
	urldate = {2018-04-12},
	date = {2016-11},
	langid = {english},
	keywords = {Anomaly detection, Isolation forest, Outlier detection},
	file = {2016-Zhu-Commentary.pdf:C\:\\Users\\nache\\Zotero\\storage\\4559S9UU\\2016-Zhu-Commentary.pdf:application/pdf}
}

@inproceedings{sanchez_statistical_2013,
	title = {Statistical selection of congruent subspaces for mining attributed graphs},
	pages = {647--656},
	booktitle = {2013 {IEEE} 13th International Conference on Data Mining},
	publisher = {{IEEE}},
	author = {Sánchez, Patricia Iglesias and Müller, Emmanuel and Laforet, Fabian and Keller, Fabian and Böhm, Klemens},
	date = {2013},
	keywords = {data mining, attribute information, attributed graph mining algorithm, attributed graphs, Communities, community outlier detection, congruent subspaces, Data mining, edge structure, Estimation, full attribute space, graph structure, graph theory, homophily, Image edge detection, Monte Carlo methods, multivariate spaces, outlier ness scores, Social network services, statistical selection method, statistical test, statistical testing, subspace selection, Vectors},
	file = {2013-Sánchez-Statistical_Selection_of_Congruent_Subspaces_for_Mining_Attributed_Graphs.pdf:C\:\\Users\\nache\\Zotero\\storage\\VM9AXFJ5\\2013-Sánchez-Statistical_Selection_of_Congruent_Subspaces_for_Mining_Attributed_Graphs.pdf:application/pdf}
}

@article{liu_isolation-based_2012,
	title = {Isolation-based anomaly detection},
	volume = {6},
	pages = {3},
	number = {1},
	journaltitle = {{ACM} Transactions on Knowledge Discovery from Data ({TKDD})},
	author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
	date = {2012},
	keywords = {Anomaly detection, outlier detection, ensemble methods, binary tree, isolation, isolation forest, random tree ensemble},
	file = {2012-Liu-Isolation-based_anomaly_detection.pdf:C\:\\Users\\nache\\Zotero\\storage\\4LPFZYXK\\2012-Liu-Isolation-based_anomaly_detection.pdf:application/pdf;Snapshot:C\:\\Users\\nache\\Zotero\\storage\\M7F6WEK9\\citation.html:text/html}
}

@inproceedings{liu_isolation_2008,
	title = {Isolation forest},
	pages = {413--422},
	booktitle = {2008 Eighth {IEEE} International Conference on Data Mining},
	publisher = {{IEEE}},
	author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
	date = {2008},
	keywords = {data mining, learning (artificial intelligence), outlier detection, anomaly detection, Data mining, isolation forest, Application software, Astronomy, binary trees, computational complexity, Constraint optimization, Credit cards, Detectors, {iForest} method, Information technology, isolation forest method, Isolation technology, Laboratories, linear time complexity, {LOF}, model based, model-based anomaly detection approach, novelty detection, {ORCA}, Performance evaluation, random forest, training data, trees (mathematics)},
	file = {2008-Liu-Isolation_Forest.pdf:C\:\\Users\\nache\\Zotero\\storage\\A28Q4BIH\\2008-Liu-Isolation_Forest.pdf:application/pdf}
}

@inproceedings{gao_converting_2006,
	title = {Converting output scores from outlier detection algorithms into probability estimates},
	pages = {212--221},
	booktitle = {Sixth International Conference on Data Mining ({ICDM}'06)},
	publisher = {{IEEE}},
	author = {Gao, Jing and Tan, Pang-Ning},
	date = {2006},
	file = {2006-Gao-Converting_output_scores_from_outlier_detection_algorithms_into_probability.pdf:C\:\\Users\\nache\\Zotero\\storage\\NZ3DHMC8\\2006-Gao-Converting_output_scores_from_outlier_detection_algorithms_into_probability.pdf:application/pdf}
}

@article{pevny_loda:_2016,
	title = {Loda: Lightweight on-line detector of anomalies},
	volume = {102},
	shorttitle = {Loda},
	pages = {275--304},
	number = {2},
	journaltitle = {Machine Learning},
	author = {Pevnỳ, Tomáš},
	date = {2016},
	keywords = {Anomaly Detector, Concept Drift, Local Outlier, Local Outlier Factor, Random Projection},
	file = {2016-Pevnỳ-Loda.pdf:C\:\\Users\\nache\\Zotero\\storage\\LYDH42FQ\\2016-Pevnỳ-Loda.pdf:application/pdf;Full Text:C\:\\Users\\nache\\Zotero\\storage\\39ER6IM3\\s10994-015-5521-0.html:text/html}
}

@inproceedings{sathe_subspace_2016,
	title = {Subspace outlier detection in linear time with randomized hashing},
	pages = {459--468},
	booktitle = {2016 {IEEE} 16th International Conference on Data Mining ({ICDM})},
	publisher = {{IEEE}},
	author = {Sathe, Saket and Aggarwal, Charu C.},
	date = {2016},
	keywords = {Training, data analysis, computational complexity, Detectors, Complexity theory, Data models, Detection algorithms, Electronic mail, file organisation, linear complexity, randomized hashing, Robustness, subspace outlier detection algorithm},
	file = {2016-Sathe-Subspace_Outlier_Detection_in_Linear_Time_with_Randomized_Hashing.pdf:C\:\\Users\\nache\\Zotero\\storage\\5UAYC4FW\\2016-Sathe-Subspace_Outlier_Detection_in_Linear_Time_with_Randomized_Hashing.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\nache\\Zotero\\storage\\6KMP9T2M\\7837870.html:text/html;Snapshot:C\:\\Users\\nache\\Zotero\\storage\\9RHMI94S\\7837870.html:text/html}
}

@article{breiman_bagging_1996,
	title = {Bagging predictors},
	volume = {24},
	pages = {123--140},
	number = {2},
	journaltitle = {Machine learning},
	author = {Breiman, Leo},
	date = {1996},
	file = {1996-Breiman-Bagging_predictors.pdf:C\:\\Users\\nache\\Zotero\\storage\\Q7QFQ3CB\\1996-Breiman-Bagging_predictors.pdf:application/pdf}
}

@inproceedings{buhlmann_bagging_2003,
	title = {Bagging, subagging and bragging for improving some prediction algorithms},
	volume = {113},
	booktitle = {Research report/Seminar für Statistik, Eidgenössische Technische Hochschule ({ETH})},
	publisher = {Seminar für Statistik, Eidgenössische Technische Hochschule ({ETH}), Zürich},
	author = {Bühlmann, Peter Lukas},
	date = {2003},
	file = {2003-Bühlmann-Bagging,_subagging_and_bragging_for_improving_some_prediction_algorithms.pdf:C\:\\Users\\nache\\Zotero\\storage\\HQHD6YLU\\2003-Bühlmann-Bagging,_subagging_and_bragging_for_improving_some_prediction_algorithms.pdf:application/pdf}
}

@article{buja_observations_2006,
	title = {Observations on bagging},
	volume = {16},
	pages = {323},
	number = {2},
	journaltitle = {Statistica Sinica},
	author = {Buja, Andreas and Stuetzle, Werner},
	date = {2006},
	file = {2006-Buja-Observations_on_bagging.pdf:C\:\\Users\\nache\\Zotero\\storage\\V36G483H\\2006-Buja-Observations_on_bagging.pdf:application/pdf}
}

@article{buhlmann_analyzing_2002,
	title = {Analyzing bagging},
	volume = {30},
	pages = {927--961},
	number = {4},
	journaltitle = {The Annals of Statistics},
	author = {Bühlmann, Peter and Yu, Bin},
	date = {2002},
	keywords = {classification, Bootstrap, decision tree, {MARS}, model selection, multiple predictions, nonparametric regression},
	file = {2002-Bühlmann-Analyzing_bagging.pdf:C\:\\Users\\nache\\Zotero\\storage\\CX7DNJCH\\2002-Bühlmann-Analyzing_bagging.pdf:application/pdf}
}

@article{aggarwal_outlier_2013,
	title = {Outlier Ensembles: Position Paper},
	volume = {14},
	issn = {1931-0145},
	url = {http://doi.acm.org/10.1145/2481244.2481252},
	doi = {10.1145/2481244.2481252},
	shorttitle = {Outlier Ensembles},
	abstract = {Ensemble analysis is a widely used meta-algorithm for many data mining problems such as classification and clustering. Numerous ensemble-based algorithms have been proposed in the literature for these problems. Compared to the clustering and classification problems, ensemble analysis has been studied in a limited way in the outlier detection literature. In some cases, ensemble analysis techniques have been implicitly used by many outlier analysis algorithms, but the approach is often buried deep into the algorithm and not formally recognized as a general-purpose meta-algorithm. This is in spite of the fact that this problem is rather important in the context of outlier analysis. This paper discusses the various methods which are used in the literature for outlier ensembles and the general principles by which such analysis can be made more effective. A discussion is also provided on how outlier ensembles relate to the ensemble-techniques used commonly for other data mining problems.},
	pages = {49--58},
	number = {2},
	journaltitle = {{SIGKDD} Explor. Newsl.},
	author = {Aggarwal, Charu C.},
	urldate = {2018-03-28},
	date = {2013-04},
	file = {2013-Aggarwal-Outlier_Ensembles.pdf:C\:\\Users\\nache\\Zotero\\storage\\GUQ8KT8I\\2013-Aggarwal-Outlier_Ensembles.pdf:application/pdf}
}

@article{rayana_less_2016,
	title = {Less is more: Building selective anomaly ensembles},
	volume = {10},
	shorttitle = {Less is more},
	pages = {42},
	number = {4},
	journaltitle = {{ACM} Transactions on Knowledge Discovery from Data ({TKDD})},
	author = {Rayana, Shebuti and Akoglu, Leman},
	date = {2016},
	keywords = {Computer Science - Machine Learning, Computer Science - Databases},
	file = {2016-Rayana-Less_is_more.pdf:C\:\\Users\\nache\\Zotero\\storage\\VKZS4JTB\\2016-Rayana-Less_is_more.pdf:application/pdf}
}

@inproceedings{tan_fast_2011,
	title = {Fast anomaly detection for streaming data},
	booktitle = {Twenty-Second International Joint Conference on Artificial Intelligence},
	author = {Tan, Swee Chuan and Ting, Kai Ming and Liu, Tony Fei},
	date = {2011},
	file = {2011-Tan-Fast_Anomaly_Detection_for_Streaming_Data.pdf:C\:\\Users\\nache\\Zotero\\storage\\YRLFPI6F\\2011-Tan-Fast_Anomaly_Detection_for_Streaming_Data.pdf:application/pdf}
}

@inproceedings{salehi_relevance_2014,
	title = {A relevance weighted ensemble model for anomaly detection in switching data streams},
	pages = {461--473},
	booktitle = {Pacific-Asia Conference on Knowledge Discovery and Data Mining},
	publisher = {Springer},
	author = {Salehi, Mahsa and Leckie, Christopher A. and Moshtaghi, Masud and Vaithianathan, Tharshan},
	date = {2014},
	keywords = {Anomaly detection, Data streams, Ensemble models},
	file = {2014-Salehi-A_Relevance_Weighted_Ensemble_Model_for_Anomaly_Detection_in_Switching_Data.pdf:C\:\\Users\\nache\\Zotero\\storage\\S3QT5AZQ\\2014-Salehi-A_Relevance_Weighted_Ensemble_Model_for_Anomaly_Detection_in_Switching_Data.pdf:application/pdf;2014-Salehi-A_Relevance_Weighted_Ensemble_Model_for_Anomaly_Detection_in_Switching_Data.pdf:C\:\\Users\\nache\\Zotero\\storage\\FY2QTZBU\\2014-Salehi-A_Relevance_Weighted_Ensemble_Model_for_Anomaly_Detection_in_Switching_Data.pdf:application/pdf;2014-Salehi-A_Relevance_Weighted_Ensemble_Model_for_Anomaly_Detection_in_Switching_Data.pdf:C\:\\Users\\nache\\Zotero\\storage\\GSRPBW9P\\2014-Salehi-A_Relevance_Weighted_Ensemble_Model_for_Anomaly_Detection_in_Switching_Data.pdf:application/pdf;Snapshot:C\:\\Users\\nache\\Zotero\\storage\\AXWP2ECF\\978-3-319-06605-9_38.html:text/html}
}

@inproceedings{nguyen_mining_2010,
	title = {Mining outliers with ensemble of heterogeneous detectors on random subspaces},
	pages = {368--383},
	booktitle = {International Conference on Database Systems for Advanced Applications},
	publisher = {Springer},
	author = {Nguyen, Hoang Vu and Ang, Hock Hee and Gopalkrishnan, Vivekanand},
	date = {2010},
	file = {2010-Nguyen-Mining_outliers_with_ensemble_of_heterogeneous_detectors_on_random_subspaces.pdf:C\:\\Users\\nache\\Zotero\\storage\\GE2GKKX7\\2010-Nguyen-Mining_outliers_with_ensemble_of_heterogeneous_detectors_on_random_subspaces.pdf:application/pdf}
}

@book{zhou_ensemble_2012,
	title = {Ensemble methods: foundations and algorithms},
	shorttitle = {Ensemble methods},
	publisher = {Chapman and Hall/{CRC}},
	author = {Zhou, Zhi-Hua},
	date = {2012},
	file = {2012-Zhou-Ensemble_methods.pdf:C\:\\Users\\nache\\Zotero\\storage\\23SUA6RQ\\2012-Zhou-Ensemble_methods.pdf:application/pdf}
}

@book{aggarwal_outlier_2017,
	title = {Outlier Ensembles: An Introduction},
	isbn = {978-3-319-54765-7},
	shorttitle = {Outlier Ensembles},
	abstract = {This book discusses a variety of methods for outlier ensembles and organizes them by the specific principles with which accuracy improvements are achieved. In addition, it covers the techniques with which such methods can be made more effective. A formal classification of these methods is provided, and the circumstances in which they work well are examined. The authors cover how outlier ensembles relate (both theoretically and practically) to the ensemble techniques used commonly for other data mining problems like classification. The similarities and (subtle) differences in the ensemble techniques for the classification and outlier detection problems are explored. These subtle differences do impact the design of ensemble algorithms for the latter problem. This book can be used for courses in data mining and related curricula. Many illustrative examples and exercises are provided in order to facilitate classroom teaching. A familiarity is assumed to the outlier detection problem and also to generic problem of ensemble analysis in classification. This is because many of the ensemble methods discussed in this book are adaptations from their counterparts in the classification domain. Some techniques explained in this book, such as wagging, randomized feature weighting, and geometric subsampling, provide new insights that are not available elsewhere. Also included is an analysis of the performance of various types of base detectors and their relative effectiveness. The book is valuable for researchers and practitioners for leveraging ensemble methods into optimal algorithmic design.},
	pagetotal = {288},
	publisher = {Springer},
	author = {Aggarwal, Charu C. and Sathe, Saket},
	date = {2017-04-06},
	langid = {english},
	note = {Google-Books-{ID}: {UNmfDgAAQBAJ}},
	keywords = {Computers / Information Technology, Computers / Intelligence ({AI}) \& Semantics, Computers / Mathematical \& Statistical Software, Computers / Networking / General, Computers / Online Services},
	file = {2017-Aggarwal-Outlier_Ensembles.pdf:C\:\\Users\\nache\\Zotero\\storage\\XX3JT77F\\2017-Aggarwal-Outlier_Ensembles.pdf:application/pdf}
}

@book{aggarwal_outlier_2017-1,
	location = {New York},
	title = {Outlier Analysis},
	isbn = {978-1-4614-6395-5},
	url = {//www.springer.com/us/book/9781461463955},
	abstract = {With the increasing advances in hardware technology for data collection, and advances in software technology (databases) for data organization, computer scientists have increasingly participated in the latest advancements of the outlier analysis field. Computer scientists, specifically, approach this field based on their practical experiences in managing large amounts of data, and with far fewer assumptions– the data can be of any type, structured or unstructured, and may be extremely large. Outlier Analysis is a comprehensive exposition, as understood by data mining experts, statisticians and computer scientists. The book has been organized carefully, and emphasis was placed on simplifying the content, so that students and practitioners can also benefit. Chapters will typically cover one of three areas: methods and techniques commonly used in outlier analysis, such as linear methods, proximity-based methods, subspace methods, and supervised methods; data domains, such as, text, categorical, mixed-attribute, time-series, streaming, discrete sequence, spatial and network data; and key applications of these methods as applied to diverse domains such as credit card fraud detection, intrusion detection, medical diagnosis, earth science, web log analytics, and social network analysis are covered.},
	publisher = {Springer-Verlag},
	author = {Aggarwal, Charu C.},
	urldate = {2018-03-20},
	date = {2017},
	langid = {english},
	file = {2017-Aggarwal-Outlier_Analysis.pdf:C\:\\Users\\nache\\Zotero\\storage\\Y5AUTJBS\\2017-Aggarwal-Outlier_Analysis.pdf:application/pdf}
}

@misc{obermayer_proof_nodate,
	title = {Proof of the Key Theorem of Statistical Learning Theory},
	url = {https://www.ni.tu-berlin.de/fileadmin/fg215/teaching/mi1/absolute_consistency.pdf},
	author = {{Obermayer}}
}

@misc{juanjo_nieto_apuntes_2018,
	title = {Apuntes Modelos Matemáticos 2},
	url = {https://www.ugr.es/~jjmnieto/docencia-files/Apuntes_sin_verficar_parte_I.pdf},
	author = {{Juanjo Nieto} and {Antonia Delgado}},
	date = {2018},
	langid = {spanish}
}

@article{jurgen_braun_constructive_nodate,
	title = {On a constructive proof of Kolmogorov's superposition theorem},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.5436&rep=rep1&type=pdf},
	pages = {19},
	author = {{Jürgen Braun} and {Michael Griebel}},
	langid = {english}
}

@book{hastie_t._elements_nodate,
	title = {The elements of Statistical Learning: Data Mining Inference and Prediction},
	publisher = {New York: Springer},
	author = {{Hastie T.} and {R. Tibshirani} and {J. Friedman}}
}

@book{vapnik_v._nature_nodate,
	title = {The Nature of Statistical Learning Theory},
	publisher = {New York: Springer},
	author = {Vapnik V.},
	langid = {english}
}

@article{nguyen_near-linear_2014,
	title = {A Near-Linear Time Subspace Search Scheme for Unsupervised Selection of Correlated Features},
	volume = {1},
	issn = {2214-5796},
	url = {http://www.sciencedirect.com/science/article/pii/S2214579614000057},
	doi = {10.1016/j.bdr.2014.07.004},
	series = {Special Issue on Scalable Computing for Big Data},
	abstract = {In many real-world applications, data is collected in high dimensional spaces. However, not all dimensions are relevant for data analysis. Instead, interesting knowledge is hidden in correlated subsets of dimensions (i.e., subspaces of the original space). Detecting these correlated subspaces independently of the underlying mining task is an open research problem. It is challenging due to the exponential search space. Existing methods have tried to tackle this by utilizing Apriori search schemes. However, their worst case complexity is exponential in the number of dimensions; and even in practice they show poor scalability while missing high quality subspaces. This paper features a scalable subspace search scheme (4S), which overcomes the efficiency problem by departing from the traditional levelwise search. We propose a new generalized notion of correlated subspaces which gives way to transforming the search space to a correlation graph of dimensions. We perform a direct mining of correlated subspaces in this graph, and then, merge subspaces based on the {MDL} principle in order to obtain high dimensional subspaces with minimal redundancy. We theoretically show that our search scheme is more general than existing search schemes. Our empirical results reveal that 4S in practice scales near-linearly with both database size and dimensionality, and produces higher quality subspaces than state-of-the-art methods.},
	pages = {37--51},
	journaltitle = {Big Data Research},
	shortjournal = {Big Data Research},
	author = {Nguyen, Hoang-Vu and Müller, Emmanuel and Böhm, Klemens},
	urldate = {2019-07-29},
	date = {2014-08-01},
	note = {00002},
	keywords = {Classification, Clustering, Correlation, Outlier mining, Subspace search, Unsupervised feature selection},
	file = {ScienceDirect Snapshot:C\:\\Users\\nache\\Zotero\\storage\\J6MLCBDP\\S2214579614000057.html:text/html}
}

@book{m._loeve_probability_1977,
	title = {Probability Theory},
	publisher = {Springer-Verlag},
	author = {M. Loève},
	date = {1977}
}

@article{fabian_keller_hics:_2012,
	title = {{HiCS}: High Constrast Subspaces for Density-Based Outlier Ranking},
	url = {https://ieeexplore.ieee.org/document/6228154},
	doi = {10.1109/ICDE.2012.88},
	abstract = {Outlier mining is a major task in data analysis. Outliers are objects that highly deviate from regular objects in their local neighborhood. Density-based outlier ranking methods score each object based on its degree of deviation. In many applications, these ranking methods degenerate to random listings due to low contrast between outliers and regular objects. Outliers do not show up in the scattered full space, they are hidden in multiple high contrast subspace projections of the data. Measuring the contrast of such subspaces for outlier rankings is an open research challenge. In this work, we propose a novel subspace search method that selects high contrast subspaces for density-based outlier ranking. It is designed as pre-processing step to outlier ranking algorithms. It searches for high contrast subspaces with a significant amount of conditional dependence among the subspace dimensions. With our approach, we propose a first measure for the contrast of subspaces. Thus, we enhance the quality of traditional outlier rankings by computing outlier scores in high contrast projections only. The evaluation on real and synthetic data shows that our approach outperforms traditional dimensionality reduction techniques, naive random projections as well as state-of-the-art subspace search techniques and provides enhanced quality for outlier ranking.},
	pages = {12},
	author = {Fabian Keller and Emmanuel Müller and Klemens Böhm},
	date = {2012},
	langid = {english}
}

@misc{shebuti_ryana_odds_2016,
	title = {{ODDS} Library},
	url = {http://odds.cs.stonybrook.edu/},
	author = {Shebuti Ryana},
	date = {2016},
	langid = {english}
}